---
title: "Analyze citation data from the crossref dataset"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
    self_contained: true
    code_download: true
---

# TODO

- if covid/metas were excluded, these would need to be excluded from total citation counts too or the proportion of citations is underestimated.

# Data source

Data was taken from the March 2025 Public Data File from Crossref: https://academictorrents.com/details/e0eda0104902d61c025e27e4846b66491d4c9f98 

The file is about 200GB so is too large to include the the public github, but can be freely downloaded from the torrent. Place the jsonl.gz files in the data/raw/ directory, there is no need to decompress them.

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, include = TRUE, warning = FALSE, message = FALSE)

# disable scientific notation
options(scipen = 999) 
```

# Dependencies

```{r}

library(tidyverse)
library(scales)
library(janitor)
library(knitr)
library(kableExtra)

```

# Descriptive statistics

## Overall

```{r}

# check saved data loads
data_processed <- read_csv("../data/processed/data_processed_subset_shuffled.csv") 

n_articles_meeting_citation_criterion <- data_processed |>
  count() |>
  pull(n)

data_logs <- read_csv("../data/processed/data_log.csv")

n_files_processed <- data_logs |>
  count() |>
  pull(n)

n_files_to_be_processed <- list.files(path = "../data/raw/", pattern = "\\.jsonl\\.gz$", full.names = TRUE) |>
  length()

n_articles_meeting_other_criteria <- data_logs |>
  summarize(total_n = sum(n_articles)) |>
  pull(total_n)

percent_articles_meeting_citation_criterion <- n_articles_meeting_citation_criterion / n_articles_meeting_other_criteria * 100

```

.jsonl files to be processed: `r n_files_to_be_processed`

.jsonl files processed: `r n_files_processed`

Articles in psych journals published after 2015: `r n_articles_meeting_other_criteria`

Articles meeting citations per year criterion: `r n_articles_meeting_citation_criterion`

Percent of articles meeting the journals and year criteria that also met the 30 citations per year criterion: `r janitor::round_half_up(percent_articles_meeting_citation_criterion, 2)`%

```{r}

citations_total <- data_logs |>
  summarize(citations_total = sum(citations_total)) |>
  pull(citations_total)

citations_articles_meeting_citation_criterion <- data_processed |>
  summarize(citations_sample = sum(citations)) |>
  pull(citations_sample)

```

Citations of all psych articles since 2015: `r citations_total`

Citations of our subset of articles with 30+ citations per year: `r citations_articles_meeting_citation_criterion`

Percent of all citations made up by our subset: `r round_half_up(citations_articles_meeting_citation_criterion/citations_total*100, 1)`%

As such, our sample of just `r janitor::round_half_up(percent_articles_meeting_citation_criterion, 2)`% of articles represented `r round_half_up(citations_articles_meeting_citation_criterion/citations_total*100, 1)`% of all citations.

## Subfield

```{r}

data_processed |>
  count(category) |>
  arrange(desc(n)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Journal

```{r}

data_processed |>
  count(journal) |>
  arrange(desc(n)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Gelman-and-King-esque plots

Gelman and King's recommendation: "Publications with at least 250 citations". However, our search strategy includes only returns articles that **also** meet the 30 citations per year since publication rule, so the preselection in the search may distort it. As such, the first plot is not a good choice for communication. 

The second one is standardized by N all articles and N all citations in the full dataset, so is more appropriate.

```{r}

data_ranks <- data_processed |>
  arrange(desc(citations)) |>
  mutate(citations_rank = row_number(),
         citations_rank_proportion = citations_rank/n_articles_meeting_other_criteria,
         citations_cumsum = cumsum(citations),
         citations_cumsum_proportion = citations_cumsum/citations_total) |>
  select(citations_rank, citations_rank_proportion, citations, citations_cumsum, citations_cumsum_proportion)

ggplot(data_ranks, aes(citations_rank, citations)) +
  geom_line() +
  geom_vline(xintercept = 1159, linetype = "dashed", color = "cyan3") + # from data_ranks where citations >= 250
  theme_linedraw() +
  ylab("Citations") +
  xlab("Publication rank") +
  coord_cartesian(xlim = c(0, 2000))

ggplot(data_ranks, aes(citations_rank_proportion, citations_cumsum_proportion)) +
  geom_line() +
  scale_x_continuous(labels = label_percent(accuracy = 0.01),
                     breaks = breaks_pretty(n = 9),
                     name = "Percent of all publications") +
  scale_y_continuous(labels = label_percent(accuracy = 1),
                     breaks = breaks_pretty(n = 8),
                     name = "Percent of all citations") +
  coord_cartesian(xlim = c(0, 0.0031)) +
  theme_linedraw() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  labs(title = "Citations of all articles in psychology journals", 
       subtitle = "645,164 articles between 2015-2025")

dir.create("plots")

ggsave("plots/citations_curve.png", width = 6, height = 4, dpi = 300)

```

# COVID studies and meta-analyses

## COVID in title

```{r}

data_processed |>
  summarize(percent_covid_in_title = round_half_up(mean(str_detect(title, regex("covid", ignore_case = TRUE)))*100, 1)) |>
  kable() |>
  kable_classic(full_width = FALSE)

data_processed |>
  filter(!str_detect(title, regex("covid", ignore_case = TRUE))) |>
  count(name = "n_without_covid")

```

## Meta in title

```{r}

data_processed |>
  summarize(percent_covid_in_title = round_half_up(mean(str_detect(title, regex("meta", ignore_case = TRUE)))*100, 1)) |>
  kable() |>
  kable_classic(full_width = FALSE)

data_processed |>
  filter(!str_detect(title, regex("meta", ignore_case = TRUE))) |>
  count(name = "n_without_covid")

```

## Meta in title excluding covid

```{r}

data_processed |>
  filter(!str_detect(title, regex("covid", ignore_case = TRUE))) |>
  summarize(percent_covid_in_title = round_half_up(mean(str_detect(title, regex("meta", ignore_case = TRUE)))*100, 1)) |>
  kable() |>
  kable_classic(full_width = FALSE)

data_processed |>
  filter(!str_detect(title, regex("covid", ignore_case = TRUE)) &
           !str_detect(title, regex("meta", ignore_case = TRUE))) |>
  count(name = "n_without_covid_or_meta")

```

# Session info

```{r}

sessionInfo()

```

