---
title: "Extracting article data from the crossref dataset"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
    self_contained: true
    code_download: true
---

# TODO

- XX

# Data source

Data was taken from the March 2025 Public Data File from Crossref: https://academictorrents.com/details/e0eda0104902d61c025e27e4846b66491d4c9f98 

The file is about 200GB so is too large to include the the public github, but can be freely downloaded from the torrent. Place the jsonl.gz files in the data/raw/ directory, there is no need to decompress them.

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, include = TRUE, warning = FALSE, message = FALSE)
```

# Dependencies

```{r}

library(tidyverse)
library(janitor)
library(jsonlite)
library(knitr)
library(kableExtra)

Sys.setenv(VROOM_CONNECTION_SIZE = 5000000)  # Allow 5MB lines

```

# Load journal ISSNs

```{r}

issn_ref <- read_csv("../data/journals/processed_journal_issns.csv") |>
  select(ISSN, impact_factor_rank, category)

```

# Process all files

- In the log file, n_articles refers to the number of DOIs published in one of the predetermined ISSNs and with a publication date after 2015. Ie total number of psychology articles per jsonl file prior to filtering for meeting the citations by year criterion. This count could be useful when reporting how many articles in total were screened, and what percentile the articles per year criterion referred to.  

```{r}

data_logs <- read_csv("../data/processed/data_log.csv")

data_logs |>
  summarize(prop_empty = mean(n_articles == 0))

# Parameters
pubyear_search_end <- 2025
raw_dir            <- "../data/raw"
outfile            <- "../data/processed/data_all_psych_doi_citations.csv"
logfile            <- "../data/processed/data_all_psych_doi_citations_log.csv"

# Initialize or read the processed‐files log
if (!file.exists(logfile)) {
  write_csv(
    tibble(file       = character()),
    logfile,
    col_names = TRUE   # write header now
  )
}
processed_log <- read_csv(logfile, col_types = cols(.default = "c"))

# Determine which files still need processing
all_files         <- list.files(raw_dir, pattern = "\\.jsonl\\.gz$", full.names = TRUE)
files_to_process  <- setdiff(all_files, processed_log$file)

if (length(files_to_process) == 0) {
  message("No new files to process.")
  quit(status = 0)
}

# Track whether we've already written headers to outfile
header_written <- file.exists(outfile)

for (file in files_to_process) {
  message("Processing: ", file)
  
  # 1) Read raw lines and parse JSON
  lines  <- read_lines(file)
  parsed <- map(lines, ~ tryCatch(fromJSON(.x, simplifyVector = FALSE),
                                  error = function(e) NULL)) %>% compact()
  if (length(parsed) == 0) next
  
  # 2) Extract “simple” scalar fields
  simple_records <- map(parsed, ~ keep(.x, ~ !is.list(.)) %>% as_tibble())
  data_simple    <- bind_rows(simple_records)

  # 3) Extract complex fields manually
  issns            <- map_chr(parsed, ~ .x$ISSN[[1]] %||% NA_character_)
  container_titles <- map_chr(parsed, ~ .x$`container-title`[[1]] %||% NA_character_)

  pub_years <- map_chr(parsed, function(record) {
    extract_year <- function(dl) {
      if (!is.null(dl$`date-parts`) && length(dl$`date-parts`) > 0) {
        as.character(dl$`date-parts`[[1]][1])
      } else NA_character_
    }
    if (!is.null(record$`published-print`))       extract_year(record$`published-print`)
    else if (!is.null(record$`published-online`)) extract_year(record$`published-online`)
    else if (!is.null(record$issued))             extract_year(record$issued)
    else NA_character_
  })

  # 4) Combine into one data frame and join ISSN reference
  data_processed <- data_simple |>
    mutate(
      ISSN           = issns,
      journal = container_titles,
      year           = pub_years
    ) |>
    select(
      ISSN,
      journal,
      DOI,
      year,
      citations = `is-referenced-by-count`
    ) |>
    mutate(
      citations = as.numeric(citations),
      year      = as.numeric(year)
    ) |>
    # filter articles after 2015, so they're published in the last decade
    filter(year >= 2015) |>
    # join impact factor and category
    left_join(issn_ref, by = "ISSN") |>
    filter(!is.na(category))
  
  # 5) Write to the master CSV
  if (nrow(data_processed) > 0) {
    write_csv(
      data_processed,
      outfile,
      append    = header_written,
      col_names = !header_written
    )
    header_written <- TRUE
  }
  
  # 6) Log this file as completed
  write_csv(tibble(file = file),
            logfile,
            append    = TRUE,
            col_names = FALSE)
}

```

# Descriptives

```{r}

# check saved data loads
data_processed <- read_csv("../data/processed/data_all_psych_doi_citations.csv") |>
  filter(more_than_30_citations_per_year)

n_articles_meeting_citation_criterion <- data_processed |>
  count() |>
  pull(n)

data_logs <- read_csv("../data/processed/data_all_psych_doi_citations_log.csv")

n_files_processed <- data_logs |>
  count() |>
  pull(n)

n_files_to_be_processed <- list.files(path = "../data/raw/", pattern = "\\.jsonl\\.gz$", full.names = TRUE) |>
  length()

n_articles_meeting_other_criteria <- data_logs |>
  summarize(total_n = sum(n_articles)) |>
  pull(total_n)

percent_articles_meeting_citation_criterion <- n_articles_meeting_citation_criterion / n_articles_meeting_other_criteria * 100

```

.jsonl files to be processed: `r n_files_to_be_processed`

.jsonl files processed: `r n_files_processed`

Articles in psych journals published after 2015: `r n_articles_meeting_other_criteria`

Articles meeting citations per year criterion: `r n_articles_meeting_citation_criterion`

Percent of articles meeting the journals and year criteria that also met the 30 citations per year criterion: `r janitor::round_half_up(percent_articles_meeting_citation_criterion, 2)`%

```{r}

citations_total <- data_logs |>
  summarize(citations_total = sum(citations_total)) |>
  pull(citations_total)

citations_articles_meeting_citation_criterion <- data_processed |>
  summarize(citations_sample = sum(citations)) |>
  pull(citations_sample)

```

Citations of all psych articles since 2015: `r citations_total`

Citations of our subset of articles with 30+ citations per year: `r citations_articles_meeting_citation_criterion`

Percent of all citations made up by our subset: `r round_half_up(citations_articles_meeting_citation_criterion/citations_total*100, 1)`%

As such, our sample of just `r janitor::round_half_up(percent_articles_meeting_citation_criterion, 2)`% of articles represented `r round_half_up(citations_articles_meeting_citation_criterion/citations_total*100, 1)`% of all citations.

```{r}

data_processed |>
  count(journal) |>
  arrange(desc(n)) |>
  kable() |>
  kable_classic(full_width = FALSE)

data_processed |>
  count(category) |>
  arrange(desc(n)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Session info

```{r}

sessionInfo()

```

