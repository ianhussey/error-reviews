---
title: "Extracting article data from the crossref dataset"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
    self_contained: true
    code_download: true
---

# TODO

- should we drop covid and metas? all the below changes if so

# Data source

Data was taken from the March 2025 Public Data File from Crossref: https://academictorrents.com/details/e0eda0104902d61c025e27e4846b66491d4c9f98 

The file is about 200GB so is too large to include the the public github, but can be freely downloaded from the torrent. Place the jsonl.gz files in the data/raw/ directory, there is no need to decompress them.

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, include = TRUE, warning = FALSE, message = FALSE)
```

# Dependencies

```{r}

library(tidyverse)
library(janitor)
library(jsonlite)
library(knitr)
library(kableExtra)

Sys.setenv(VROOM_CONNECTION_SIZE = 5000000)  # Allow 5MB lines

```

# Load journal ISSNs

```{r}

issn_ref <- read_csv("../data/journals/processed_journal_issns.csv") |>
  select(ISSN, impact_factor_rank, category)

```

# Process all files

- In the log file, n_articles refers to the number of DOIs published in one of the predetermined ISSNs and with a publication date after 2015. Ie total number of psychology articles per jsonl file prior to filtering for meeting the citations by year criterion. This count could be useful when reporting how many articles in total were screened, and what percentile the articles per year criterion referred to.  

```{r}

# data_logs <- read_csv("../data/processed/data_log.csv")
# 
# data_logs |>
#   summarize(prop_empty = mean(n_articles == 0))

# Parameters
pubyear_search_end <- 2025
raw_dir            <- "../data/raw"
outfile            <- "../data/processed/data_all_psych_doi_citations.csv"
logfile            <- "../data/processed/data_all_psych_doi_citations_log.csv"

# Initialize or read the processed‐files log
if (!file.exists(logfile)) {
  write_csv(
    tibble(file       = character()),
    logfile,
    col_names = TRUE   # write header now
  )
}
processed_log <- read_csv(logfile, col_types = cols(.default = "c"))

# Determine which files still need processing
all_files         <- list.files(raw_dir, pattern = "\\.jsonl\\.gz$", full.names = TRUE)
files_to_process  <- setdiff(all_files, processed_log$file)

if (length(files_to_process) == 0) {
  message("No new files to process.")
  quit(status = 0)
}

# Track whether we've already written headers to outfile
header_written <- file.exists(outfile)

for (file in files_to_process) {
  message("Processing: ", file)
  
  # 1) Read raw lines and parse JSON
  lines  <- read_lines(file)
  parsed <- map(lines, ~ tryCatch(fromJSON(.x, simplifyVector = FALSE),
                                  error = function(e) NULL)) %>% compact()
  if (length(parsed) == 0) next
  
  # 2) Extract “simple” scalar fields
  simple_records <- map(parsed, ~ keep(.x, ~ !is.list(.)) %>% as_tibble())
  data_simple    <- bind_rows(simple_records)

  # 3) Extract complex fields manually
  issns            <- map_chr(parsed, ~ .x$ISSN[[1]] %||% NA_character_)
  container_titles <- map_chr(parsed, ~ .x$`container-title`[[1]] %||% NA_character_)

  pub_years <- map_chr(parsed, function(record) {
    extract_year <- function(dl) {
      if (!is.null(dl$`date-parts`) && length(dl$`date-parts`) > 0) {
        as.character(dl$`date-parts`[[1]][1])
      } else NA_character_
    }
    if (!is.null(record$`published-print`))       extract_year(record$`published-print`)
    else if (!is.null(record$`published-online`)) extract_year(record$`published-online`)
    else if (!is.null(record$issued))             extract_year(record$issued)
    else NA_character_
  })

  # 4) Combine into one data frame and join ISSN reference
  data_processed <- data_simple |>
    mutate(
      ISSN           = issns,
      journal = container_titles,
      year           = pub_years
    ) |>
    select(
      ISSN,
      journal,
      DOI,
      year,
      citations = `is-referenced-by-count`
    ) |>
    mutate(
      citations = as.numeric(citations),
      year      = as.numeric(year)
    ) |>
    # filter articles after 2015, so they're published in the last decade
    filter(year >= 2015) |>
    # join impact factor and category
    left_join(issn_ref, by = "ISSN") |>
    filter(!is.na(category))
  
  # 5) Write to the master CSV
  if (nrow(data_processed) > 0) {
    write_csv(
      data_processed,
      outfile,
      append    = header_written,
      col_names = !header_written
    )
    header_written <- TRUE
  }
  
  # 6) Log this file as completed
  write_csv(tibble(file = file),
            logfile,
            append    = TRUE,
            col_names = FALSE)
}

```

# Descriptives

```{r}

# check saved data loads
data_logs <- read_csv("../data/processed/data_all_psych_doi_citations_log.csv")

n_files_processed <- data_logs |>
  count() |>
  pull(n)

n_files_to_be_processed <- list.files(path = "../data/raw/", pattern = "\\.jsonl\\.gz$", full.names = TRUE) |>
  length()


data_processed <- read_csv("../data/processed/data_all_psych_doi_citations.csv") |>
  filter(year >= 2015 & year <= 2025)

n_articles <- data_processed |>
  count() |>
  pull(n)

```

.jsonl files to be processed: `r n_files_to_be_processed`

.jsonl files processed: `r n_files_processed`

Articles in psych journals published after 2015: `r n_articles`

Articles meeting citations per year criterion: `r n_articles_meeting_citation_criterion`

```{r}

citations_total <- data_processed |>
  summarize(citations_total = sum(citations)) |>
  pull(citations_total)

```

Citations of all psych articles since 2015: `r citations_total`

```{r}

data_processed |>
  count(category) |>
  arrange(desc(n)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Journal misspellings/errors

Fuzzy matching to find misspellings

```{r}

library(stringdist)

df_comp <- data_processed |>
  distinct(journal) |>
  arrange(journal) |>
  rownames_to_column() |>
  mutate(journal_prev = lag(journal),
         dist = stringdist(journal, journal_prev, method = "jw")) |> # Jaro-Winkler is good for misspellings
  arrange(dist) |>
  filter(dist < 0.09) # You can adjust this threshold

df_comp |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# N journals ignoring misspellings/errors

```{r}

data_processed |>
  count(journal) |>
  arrange(desc(n)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Distribution of citations

```{r}

ggplot(data_processed, aes(citations)) +
  geom_density() +
  theme_linedraw()

ggplot(data_processed, aes(citations)) +
  geom_density() +
  theme_linedraw() +
  scale_x_continuous(limits = c(0, 100))

ggplot(data_processed, aes(citations)) +
  geom_density() +
  theme_linedraw() +
  facet_wrap(~ year, scales = "free_y") +
  scale_x_continuous(limits = c(0, 100))

ggplot(data_processed, aes(log(citations))) +
  geom_histogram() +
  theme_linedraw() 

```




# Percentiles

```{r fig.height=4, fig.width=10}

percentiles <- tibble(
  percentile = 1:99,
  citations = map_dbl(percentile / 100, ~ quantile(data_processed$citations, probs = .x, na.rm = TRUE))
)

percentiles |>
  kable() |>
  kable_classic(full_width = FALSE)

p <- ggplot(percentiles, aes(percentile, citations)) +
  geom_bar(stat = "identity", width = 0.8) +
  theme_linedraw()


percentiles_high <- tibble(
  percentile = c(99.5, 99.9, 99.99, 99.999, 99.9999),
  citations = map_dbl(percentile / 100, ~ quantile(data_processed$citations, probs = .x, na.rm = TRUE))
)

percentiles_high |>
  kable() |>
  kable_classic(full_width = FALSE)

p_high <- ggplot(percentiles_high, aes(as.factor(percentile), citations)) +
  #geom_point() +
  geom_bar(stat = "identity", width = 0.8) +
  xlab("percentile") +
  theme_linedraw()

library(patchwork)

p + p_high + plot_layout(ncol = 2, widths = c(2.5,1))

ggsave("citations_percentiles.png", width = 10, height = 4)

```


```{r}

citations_2015 <- data_processed |>
  filter(year == 2015)

percentiles_2015_articles <- tibble(
  percentile = 1:99,
  citations = map_dbl(percentile / 100, ~ quantile(citations_2015$citations, probs = .x, na.rm = TRUE))
)

ggplot(percentiles_2015_articles, aes(percentile, citations)) +
  geom_bar(stat = "identity", width = 0.8) +
  theme_linedraw()

```



# Gelman argument

Gelman & King (2025) recently estimated from empirical study of the literature that 5% of all published articles account for 28% of all citations.

```{r}

# using gelman criterion
percentiles <- tibble(
  percentile = c(95, 99, 99.5, 99.9),
  citations = map_dbl(percentile / 100, ~ quantile(data_processed$citations, probs = .x, na.rm = TRUE))
)

percentiles |>
  kable() |>
  kable_classic(full_width = FALSE)

data_processed |>
  summarize(total_citations = sum(citations),
            total_citations_95_percentile = sum(citations[citations >= 48]),
            total_citations_99_percentile = sum(citations[citations >= 118]),
            total_citations_99.5_percentile = sum(citations[citations >= 169]),
            total_citations_99.9_percentile = sum(citations[citations >= 369])) |>
  mutate(percent_95 = total_citations_95_percentile/total_citations*100,
         percent_99 = total_citations_99_percentile/total_citations*100,
         percent_99.5 = total_citations_99.5_percentile/total_citations*100,
         percent_99.9 = total_citations_99.9_percentile/total_citations*100) |>
  select(starts_with("percent")) |>
  mutate_if(is.numeric, round_half_up, digits = 1)


# using error criterion
data_processed |>
    mutate(year_for_citations_by_year = ifelse(year > 2024, 2024, year),
           citations_per_year = janitor::round_half_up(citations / (2025 - year_for_citations_by_year), 2),
           more_than_30_citations_per_year = citations_per_year >= 30) |>
  summarize(percent_criterion_articles = round_half_up(mean(more_than_30_citations_per_year)*100, 2),
            total_citations = sum(citations),
            total_citations_error_criterion = sum(citations[more_than_30_citations_per_year])) |>
  mutate(percent_error_criterion = round_half_up(total_citations_error_criterion/total_citations*100, 1))

```

In psychology in the last decade:

- 5% of articles account for 42.2% of all citations (8.4X over-representation)
- 1% of articles account for 19.0% of all citations (19X over-representation)
- 0.5% of articles account for 13.3% of all citations (26.6X over-representation)
- 0.1% of articles account for 5.6% of all citations (56X over-representation)


# Session info

```{r}

sessionInfo()

```

