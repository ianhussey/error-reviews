# SUMMARY AND EVALUATION

The study demonstrates a relation between metacognitive responses and psychiatric symptom clusters in a non-clinical sample of Mechanical Turk workers. Depressive symptoms coincide with lower confidence ratings and obsessive-compulsive symptoms with higher confidence ratings, while task performance is unaffected.

The results are straightforward to interpret and convincing. There were only two drawbacks regarding validity: The study investigated their question in a Mechanical Turk population which makes generalization questionable. Additionally, there was a staircase procedure applied in Experiment 2 which could theoretically produce problems with interpreting metacognitive efficiency estimates. After closer inspection, neither of these issues seems had problematic consequences.

The evaluation scripts to reproduce the data are readily available and of high quality with some limitations regarding preprocessing: Drift diffusion parameter estimates and metacognition estimates were precomputed and are not directly reproducible in the freely available code. Similarly, exclusion cannot be reproduced because only the included data is freely available. However, after close inspection, it is highly likely that there are no problems from this part. Taken together, the study features a surprisingly high reliability.

I provide code for my own reproduction here: https://github.com/saschameyen/Error_Reviews_Rouault_et_al_2018_by_Sascha_Meyen

# MAJOR ISSUES

1. MECHANICAL TURK SAMPLE

My biggest concern was that the population of Mechanical Turk workers does not reflect the general population well in the relevant features for this study. To address this, the authors had already applied exclusion criteria that led to discarding 23% of the participants. The authors argue that this is within the expected range for that population. But even after this exclusion, the question remains whether the included sample deviates from the general population, especially with respect to their responses in the clinical questionnaires.

To investigate this, I took the samples of the original studies of these clinical questionnaires as a reference and compared it to the Mechanical Turk sample of Experiment 2. This yielded surprisingly comparable results. Here are three examples:

1. The original authors of the Obsessive-Compulsive Inventory-Revised (Foa et al., 2002) reported that obsessive-compulsive patients have a median score of 25. In contrast, patients with other disorders had median scores of 7 and 11. In comparison, the sample of Experiment 2 also had a median score of 11 indicating that the Mechanical Turk population does not respond higher on this questionnaire than expected.

2. The authors of the Short Scales for Measuring Schizotypy (Mason et al., 2005) reported for the non-schizotypic reference sample a mean total score of 13.0 (out of 43 points). The sample of Experiment 2 had a mean score of 12.4, which was again very comparable.

3. For the Zung Self-Rating Depression Scale (Zung, 1965), the clinically relevant group had raw scores above 48 while the sample of Experiment 2 had a mean score of 37, which is in the range of expected values for the non-clinical reference sample.

Based on these comparisons, there is no evidence that the Mechanical Turk sample of the study deviates from the general population in a meaningful way. Therefore, despite initial doubts I consider this aspect of the study not problematic. 

Foa, E.B., Huppert, J.D., Leiberg, S., Hajcak, G., Langner, R., et al. (2002). The Obsessive-Compulsive Inventory: Development and validation of a short version. Psychological Assessment, 14, 485-496.

Mason, O., Linney, Y., & Claridge, G. (2005). Short scales for measuring schizotypy. Schizophrenia Research, 78(2), 293-296.

Zung, W. W. (1965). A self-rating depression scale. Archives of General Psychiatry, 12(1), 63-70. 

2. STAIRCASE PROCEDURE AND M-RATIO

A conceptual problem arises in Experiment 2 where the authors administered a staircase procedure. The staircase procedure aims at keeping participants' accuracies constant. However, there are problems with computing metacognitive efficiency (M-Ratio) in combination with such staircase procedures, see Rahnev & Fleming (2019). The reason for this is that the volatility of the staircase procedure is a confound of M-Ratio: Consider a participant with variable performance at different phases of the perceptual task. This participant may have a relatively high performance in one block but a relatively low performance in another block. The staircase procedure will then produce difficult stimuli in one block and easy stimuli in the other. This variability in stimulus difficulty makes it easier for the participant to give appropriate confidence ratings leading to higher values of M-Ratio. In contrast, consider another participant with a relatively stable performance throughout the blocks. Because this participant always gets approximately the same difficulty of stimuli, giving appropriate confidence ratings is more difficult and leads to lower values of M-Ratio. Thus, it is possible that performance variability overshadowed the relation between M-Ratio and symptom clusters.

To investigate this, I correlated M-Ratio with the variability of the difficulty of the presented stimuli. Additionally, I correlated the symptom clusters with this performance variability. Fortuitously, there was no relevant relationship to report. Thus, again, I consider this aspect as non-problematic. Although conceptually problematic, it seems that no practical problem manifested. Another aspect that alleviates concerns here is that comparable results were found in Experiment 1 where no staircase was applied.

Rahnev, D., & Fleming, S. M. (2019). How experimental procedures influence estimates of metacognitive ability. Neuroscience of Consciousness, 2019(1), niz009.

3. UNAVAILABILITY OF METACOGNITIVE FITTING, DDM FITTING, AND EXCLUSIONS

The available code does not provide functions to fit the metacognitive efficiency scores, the drift diffusion model parameters, and to check the exclusions. Regarding the metacognitive efficiency scores, I applied a different package than the authors suggested and found highly correlated fits (slight deviations are to be expected due to different fitting procedures), so there is no problem in this regard. 

Regarding the drift diffusion fits, I was unable to reproduce the author's results within appropriate time. The MATLAB functions they referred to did no converge when I tried to apply them. To still roughly validate the results, I simulated the DDM models with the given fits and visually inspected the fits. The only issue I encountered here was a minor misfit between the non-decision time parameter and the response times provided in the freely available data set: Published response times do not include the 300 ms of stimulus presentation time. In contrast, DDM parameters seem to have been fit on response time data that include the 300 ms of stimulus presentation time. Thus, if one reproduces the DDM fits on the freely available data, the non-decision time parameters will be consistently shorter by 300 ms. Because only correlation analyses were done on these non-decision times, this creates no problematic consequences.

Regarding exclusions, I validated that participants who were included followed the exclusion criteria. Taken together, none of these omissions of reproducible steps seem problematic.

# MINOR ISSUES

There are several issues that are so minor that they are barely worth mentioning. To demonstrate the level of scrutiny necessary to find these mistakes, I anyway report them here. This is not supposed to be nitpicking: Because the only mistakes the authors made are on this level, the research community can safely trust their results.

1. On p. 1 of their supplement, they mention that stimuli in Experiment 1 were categorized into 5 (five) difficulty bins although their results in Figure 1 show 6 (six) bins. I reproduced these Figures without binning the stimulus difficulty and the data pattern is very consistent. I am confident that there is no problem with post hoc selection of bins and that this is a very minor typo.

2. In their perceptual task data in both experiments, the authors seemed to have excluded some trials, perhaps due to long response times. I believe they have failed to report this exclusion criterion for individual trials. But the exclusion rate was low with a median exclusion rate of only 1% of the trials. So this should not have created any problems.

3. It is unclear to me why their Figure S6A does not have a metacognitive efficiency comparison similar to S6B. I reproduced the results and they are similar to S6B. This seems like an inconsequential omission.

4. The authors mention using an 11-point confidence scale for Experiment 1 and then a 6-point scale for Experiment 2. Based on their analyses, it seems they have down-scaled confidences in Experiment 1 without mentioning it. Again, this is such a minor detail that it is barely worth mentioning and does not impact results.

In summary, none of these issues were problematic. Most of them I would not have mentioned in a regular review because of how inconsequential they are. Here, I applied a higher level of scrutiny, which the study passed without doubt.

####

- Signed Review, Sascha Meyen